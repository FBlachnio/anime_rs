{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639a8bdb",
   "metadata": {},
   "source": [
    "---\n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fdf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "anime_df = pd.read_parquet('Data/preprocessed_anime.parquet')\n",
    "ratings_df = pd.read_parquet('Data/preprocessed_ratings.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8234652",
   "metadata": {},
   "source": [
    "---\n",
    "Split and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e218155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_users(ratings_df, n_users=100, min_anime_rated=20, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    counts = ratings_df.groupby('userID').size()\n",
    "    pool = counts[counts >= min_anime_rated].index.values\n",
    "\n",
    "    return np.random.choice(pool, size=n_users, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a994c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def evaluate_ratings(test_df, recommendations_df, anime_df, anime_feature_matrix, anime_id_to_index, rating_threshold=7, k=10):\n",
    "\n",
    "    recommended_ids = recommendations_df.index.tolist()[:k]\n",
    "    relevant_set = set(test_df.loc[test_df['rating'] >= rating_threshold, 'idMal'].values)\n",
    "\n",
    "    # Precision and Recall\n",
    "    hits = len(set(recommended_ids) & relevant_set)\n",
    "    precision = hits / k\n",
    "    recall = hits / len(relevant_set) if relevant_set else 0\n",
    "\n",
    "    # MAP (AP)\n",
    "    avg_prec = 0\n",
    "    hits_so_far = 0\n",
    "    for i, rid in enumerate(recommended_ids, start=1):\n",
    "        if rid in relevant_set:\n",
    "            hits_so_far += 1\n",
    "            avg_prec += hits_so_far / i\n",
    "    map = avg_prec / hits if hits > 0 else 0\n",
    "\n",
    "    # MRR\n",
    "    mrr = 0\n",
    "    for i, id in enumerate(recommended_ids, start=1):\n",
    "        if id in relevant_set:\n",
    "            mrr = 1 / i\n",
    "            break\n",
    "\n",
    "    # Diversity\n",
    "    rec_indexes = [anime_id_to_index[rid] for rid in recommended_ids if rid in anime_id_to_index]\n",
    "    rec_vectors = anime_feature_matrix[rec_indexes]\n",
    "    rec_vectors = normalize(rec_vectors)\n",
    "    if len(rec_vectors) > 1:\n",
    "        sim_matrix = rec_vectors @ rec_vectors.T\n",
    "        diversity = 1 - np.mean(sim_matrix[np.triu_indices(len(rec_vectors), 1)])\n",
    "    else:\n",
    "        diversity = 0\n",
    "        \n",
    "    # Novelty\n",
    "    pop_values = anime_df.set_index('idMal').loc[recommended_ids, 'popularity'].values\n",
    "    novelty = np.mean(1 - pop_values)\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'MAP': map,\n",
    "        'MRR': mrr,\n",
    "        'Diversity': diversity,\n",
    "        'Novelty': novelty,\n",
    "        'Relevant animes': len(relevant_set),\n",
    "        'Test-set size': len(test_df)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da37d20",
   "metadata": {},
   "source": [
    "---\n",
    "CBF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2990e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Prepare columns\n",
    "anime_df_ids = anime_df['idMal'].values\n",
    "anime_df_titles = anime_df['title'].values\n",
    "\n",
    "cols_prefixes = {\n",
    "    'genres_': 'genres',\n",
    "    'tags_': 'tags',\n",
    "    'desc_': 'desc',\n",
    "    'duration_': 'cat',\n",
    "    'year_': 'cat',\n",
    "    'format_': 'cat',\n",
    "    'source_': 'cat'\n",
    "}\n",
    "\n",
    "group_cols = {group: [] for group in set(cols_prefixes.values())}\n",
    "\n",
    "for col in anime_df.columns:\n",
    "    for prefix, group in cols_prefixes.items():\n",
    "        if col.startswith(prefix):\n",
    "            group_cols[group].append(col)\n",
    "            break\n",
    "\n",
    "X = pd.concat([anime_df[cols] for cols in group_cols.values()], axis=1)\n",
    "\n",
    "group_indexes = {group: [X.columns.get_loc(c) for c in cols] for group, cols in group_cols.items()}\n",
    "\n",
    "weights = {'genres': 0.5, 'tags': 0.1,'cat': 0.4, 'desc': 0}\n",
    "\n",
    "# Create items vectors\n",
    "X_np = np.ascontiguousarray(X.values.astype('float32'))\n",
    "X_weighted = np.zeros_like(X_np)\n",
    "\n",
    "for group, ids in group_indexes.items():\n",
    "    block = X_np[:, ids]\n",
    "    norm = np.linalg.norm(block, axis=1, keepdims=True)\n",
    "    norm[norm == 0] = 1\n",
    "    X_weighted[:, ids] = (block / norm) * weights[group]\n",
    "\n",
    "faiss.normalize_L2(X_weighted)\n",
    "\n",
    "index = faiss.IndexFlatIP(X_np.shape[1])\n",
    "index.add(X_weighted)\n",
    "\n",
    "id_to_idx = {id: i for i, id in enumerate(anime_df_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c63fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def recommend_for_user_cbf(user_train, X_weighted, id_to_idx, anime_df_ids, anime_df_titles, index):\n",
    "    \n",
    "    # Create user vector\n",
    "    user_ratings = user_train['rating'].values\n",
    "\n",
    "    # if no ratings\n",
    "    if np.all(user_ratings == 0):\n",
    "        user_ratings_norm = np.ones_like(user_ratings)\n",
    "    else:\n",
    "        user_ratings_norm = user_ratings / user_ratings.max()\n",
    "    \n",
    "    seen_idx = [id_to_idx[idm] for idm in user_train['idMal'].values if idm in id_to_idx]\n",
    "    seen_vectors = X_weighted[seen_idx]\n",
    "    user_profile = np.average(seen_vectors, axis=0, weights=user_ratings_norm)\n",
    "    user_profile = np.expand_dims(user_profile, axis=0).astype('float32')\n",
    "    faiss.normalize_L2(user_profile)\n",
    "\n",
    "    # Similarity\n",
    "    distances, indexes = index.search(user_profile, index.ntotal)\n",
    "    recommended_ids = anime_df_ids[indexes.ravel()]\n",
    "    recommended_scores = distances.ravel()\n",
    "    recommended_titles = anime_df_titles[indexes.ravel()]\n",
    "\n",
    "    # Filter\n",
    "    seen_set = set(user_train['idMal'].values)\n",
    "    recs_filtered = [(rid, score, title) for rid, score, title in zip(recommended_ids, recommended_scores, recommended_titles) if rid not in seen_set]\n",
    "\n",
    "    # Result\n",
    "    recommendations_cbf_df = pd.DataFrame(recs_filtered, columns=['idMal', 'score', 'title']).set_index('idMal')\n",
    "    scaler = MinMaxScaler()\n",
    "    recommendations_cbf_df['score'] = scaler.fit_transform(recommendations_cbf_df[['score']])\n",
    "\n",
    "    return recommendations_cbf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e978d",
   "metadata": {},
   "source": [
    "---\n",
    "CF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5163a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import pickle\n",
    "\n",
    "anime_ids = ratings_df['idMal'].unique()\n",
    "anime_map = {aid: idm for idm, aid in enumerate(anime_ids)}\n",
    "user_ids = ratings_df['userID'].unique()\n",
    "user_map = {uid: idm for idm, uid in enumerate(user_ids)}\n",
    "\n",
    "rows = ratings_df['userID'].map(user_map)\n",
    "cols = ratings_df['idMal'].map(anime_map)\n",
    "vals = ratings_df['rating']\n",
    "user_item_matrix = csr_matrix((vals, (rows, cols)), shape=(len(user_map), len(anime_map)))\n",
    "\n",
    "als_model = AlternatingLeastSquares(factors=150, regularization=0.01, iterations=10)\n",
    "als_model.fit(user_item_matrix)\n",
    "\n",
    "with open('CF/als_model.pkl', 'wb') as f:\n",
    "    pickle.dump(als_model, f)\n",
    "with open('CF/anime_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(anime_ids, f)\n",
    "with open('CF/anime_map.pkl', 'wb') as f:\n",
    "    pickle.dump(anime_map, f)\n",
    "with open('CF/user_map.pkl', 'wb') as f:\n",
    "    pickle.dump(user_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58651c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fblac\\Documents\\py_proj\\Anime\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('CF/als_model.pkl','rb') as f:\n",
    "    als_model = pickle.load(f)\n",
    "with open('CF/anime_ids.pkl','rb') as f:\n",
    "    anime_ids = pickle.load(f)\n",
    "with open('CF/anime_map.pkl','rb') as f:\n",
    "    anime_map = pickle.load(f)\n",
    "with open('CF/user_map.pkl','rb') as f:\n",
    "    user_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f899f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def recommend_for_user_cf(user_train, anime_df, als_model, anime_ids, anime_map, user_id, user_map, ratings_df):\n",
    "   \n",
    "    # Create user vector\n",
    "    user_idx = user_map[user_id]\n",
    "    user_ratings = ratings_df[ratings_df['userID'] == user_id]\n",
    "    rows = np.zeros(len(user_ratings))\n",
    "    cols = user_ratings['idMal'].map(anime_map).values\n",
    "    vals = user_ratings['rating'].values\n",
    "    user_item_vector = csr_matrix((vals, (rows, cols)), shape=(1, len(anime_ids)))\n",
    "\n",
    "    # Similarity\n",
    "    recommended_indexes, recommended_scores = als_model.recommend(\n",
    "        user_idx,\n",
    "        user_items=user_item_vector,\n",
    "        N=len(anime_ids),\n",
    "        filter_already_liked_items=False\n",
    "    )\n",
    "    recommended_ids = [anime_ids[idx] for idx in recommended_indexes]\n",
    "\n",
    "    # Filter\n",
    "    seen_train_set = set(user_train['idMal'])\n",
    "    recs_filtered = [(rid, score) for rid, score in zip(recommended_ids, recommended_scores) if rid not in seen_train_set]\n",
    "\n",
    "    # Results\n",
    "    recommendations_cf_df = pd.DataFrame(recs_filtered, columns=['idMal', 'score']).set_index('idMal')\n",
    "    recommendations_cf_df = recommendations_cf_df.join(anime_df.set_index('idMal')['title'], how='left')\n",
    "    scaler = MinMaxScaler()\n",
    "    recommendations_cf_df['score'] = scaler.fit_transform(recommendations_cf_df[['score']])\n",
    "\n",
    "    return recommendations_cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfa4b8",
   "metadata": {},
   "source": [
    "---\n",
    "Hybrid Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f5d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user_hybrid(recs_cbf, recs_cf, anime_df, alpha=0.5, boost_pop=0.0, boost_score=0.0, k=20):\n",
    "\n",
    "    merged = recs_cf[['score']].rename(columns={'score': 'score_cf'}).join(\n",
    "        recs_cbf[['score']].rename(columns={'score': 'score_cbf'}),\n",
    "        how='outer'\n",
    "    ).fillna(0)\n",
    "\n",
    "    merged['score'] = alpha * merged['score_cbf'] + (1 - alpha) * merged['score_cf']\n",
    "\n",
    "    boost_df = anime_df.set_index('idMal')[['popularity', 'averageScore']]\n",
    "    merged = merged.join(boost_df, how='left').fillna(0)\n",
    "    merged['score'] += boost_pop * merged['popularity'] + boost_score * merged['averageScore']\n",
    "\n",
    "    titles = recs_cf[['title']].combine_first(recs_cbf[['title']])\n",
    "    merged = merged.join(titles, how='left')\n",
    "    \n",
    "    recommendations_hybrid_df = merged[['score', 'title']].sort_values('score', ascending=False).head(k)\n",
    "\n",
    "    return recommendations_hybrid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ca575",
   "metadata": {},
   "source": [
    "---\n",
    "K-Fold Validation - one User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e00396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_one_user_kfold_with_recs(user_id, ratings_df, als_model, user_map, anime_map, anime_ids, anime_df, X_weighted, \n",
    "                                  id_to_idx, index=None, anime_df_ids=None, anime_df_titles=None, rating_threshold=6,\n",
    "                                  alpha=0.5, boost_pop=0, boost_score=0, k_folds=5, top_k=10, seed=42):\n",
    "\n",
    "    user_ratings = ratings_df[ratings_df['userID'] == user_id].copy()\n",
    "    n_ratings = len(user_ratings)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    indexes = np.random.permutation(n_ratings)\n",
    "\n",
    "    fold_sizes = np.full(k_folds, n_ratings // k_folds, dtype=int)\n",
    "    fold_sizes[:n_ratings % k_folds] += 1\n",
    "\n",
    "    metrics_cbf = []\n",
    "    metrics_cf = []\n",
    "    metrics_hybrid = []\n",
    "    last_recommendations = None\n",
    "\n",
    "    current = 0\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        test_idx = indexes[start:stop]\n",
    "        train_idx = np.concatenate([indexes[:start], indexes[stop:]])\n",
    "\n",
    "        user_train = user_ratings.iloc[train_idx].reset_index(drop=True)\n",
    "        user_test  = user_ratings.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "        recs_cbf = recommend_for_user_cbf(user_train, X_weighted, id_to_idx, anime_df_ids, anime_df_titles, index=index)\n",
    "        recs_cf = recommend_for_user_cf(user_train, anime_df, als_model, anime_ids, anime_map, user_id, user_map, ratings_df)\n",
    "        recs_hybrid = recommend_for_user_hybrid(recs_cbf, recs_cf, anime_df, alpha, boost_pop, boost_score, top_k)\n",
    "\n",
    "        metrics_cbf.append(evaluate_ratings(user_test, recs_cbf, anime_df, X_weighted, id_to_idx, rating_threshold=rating_threshold, k=top_k))\n",
    "        metrics_cf.append(evaluate_ratings(user_test, recs_cf, anime_df, X_weighted, id_to_idx, rating_threshold=rating_threshold, k=top_k))\n",
    "        metrics_hybrid.append(evaluate_ratings(user_test, recs_hybrid, anime_df, X_weighted, id_to_idx, rating_threshold=rating_threshold, k=top_k))\n",
    "\n",
    "        last_recommendations = recs_hybrid\n",
    "        current = stop\n",
    "\n",
    "    avg_metrics = {\n",
    "        'CBF': {metric: np.mean([m[metric] for m in metrics_cbf]) for metric in metrics_cbf[0]},\n",
    "        'CF': {metric: np.mean([m[metric] for m in metrics_cf]) for metric in metrics_cf[0]},\n",
    "        'Hybrid': {metric: np.mean([m[metric] for m in metrics_hybrid]) for metric in metrics_hybrid[0]}\n",
    "    }\n",
    "\n",
    "    return avg_metrics, last_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a0d88",
   "metadata": {},
   "source": [
    "---\n",
    "K-Fold Validation - Many Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "634d3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_users_with_kfold(user_ids, ratings_df, als_model, user_map, anime_map, anime_ids, anime_df, X_weighted, id_to_idx, \n",
    "                                       index, rating_threshold=7, alpha=0.5, boost_pop=0, boost_score=0, k_folds=5, top_k=10, seed=42):\n",
    "    \n",
    "    anime_df_ids = anime_df['idMal'].values\n",
    "    anime_df_titles = anime_df['title'].values\n",
    "    \n",
    "    all_metrics_cbf = []\n",
    "    all_metrics_cf = []\n",
    "    all_metrics_hybrid = []\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        avg_metrics, _ = evaluate_one_user_kfold_with_recs(user_id, ratings_df, als_model, user_map, anime_map, anime_ids, anime_df, \n",
    "                                                       X_weighted, id_to_idx, index, anime_df_ids, anime_df_titles, rating_threshold,\n",
    "                                                       alpha, boost_pop, boost_score, k_folds, top_k, seed)\n",
    "        \n",
    "        all_metrics_cbf.append(avg_metrics['CBF'])\n",
    "        all_metrics_cf.append(avg_metrics['CF'])\n",
    "        all_metrics_hybrid.append(avg_metrics['Hybrid'])\n",
    "    \n",
    "    def average_metrics(metrics_list):\n",
    "        return {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
    "    \n",
    "    global_avg_metrics = {\n",
    "        'CBF': average_metrics(all_metrics_cbf),\n",
    "        'CF': average_metrics(all_metrics_cf),\n",
    "        'Hybrid': average_metrics(all_metrics_hybrid)\n",
    "    }\n",
    "    \n",
    "    return global_avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528721c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CBF\n",
      "Precision: 0.06\n",
      "Recall: 0.02\n",
      "MAP: 0.12\n",
      "MRR: 0.13\n",
      "Diversity: 0.15\n",
      "Novelty: 0.38\n",
      "Relevant animes: 31.59\n",
      "Test-set size: 41.99\n",
      "\n",
      "CF\n",
      "Precision: 0.58\n",
      "Recall: 0.31\n",
      "MAP: 0.70\n",
      "MRR: 0.75\n",
      "Diversity: 0.50\n",
      "Novelty: 0.20\n",
      "Relevant animes: 31.59\n",
      "Test-set size: 41.99\n",
      "\n",
      "Hybrid\n",
      "Precision: 0.52\n",
      "Recall: 0.26\n",
      "MAP: 0.71\n",
      "MRR: 0.81\n",
      "Diversity: 0.37\n",
      "Novelty: 0.17\n",
      "Relevant animes: 31.59\n",
      "Test-set size: 41.99\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for multiple users\n",
    "\n",
    "users = 500\n",
    "min_anime_rated = 50\n",
    "test_size = 0.2 \n",
    "rating_threshold = 7 \n",
    "alpha = 0.5\n",
    "boost_pop = 0.1\n",
    "boost_score = 0.1\n",
    "k_folds = 5 \n",
    "top_k = 10\n",
    "\n",
    "\n",
    "user_ids_sample = sample_users(ratings_df, users, min_anime_rated)\n",
    "\n",
    "global_results = evaluate_multiple_users_with_kfold(user_ids_sample, ratings_df, als_model, user_map, anime_map, anime_ids,anime_df, X_weighted,\n",
    "                                                    id_to_idx, index, rating_threshold, alpha, boost_pop, boost_score, k_folds, top_k)\n",
    "\n",
    "for model_name, metrics in global_results.items():\n",
    "    print(f'\\n{model_name}')\n",
    "    for metric, value in metrics.items():\n",
    "        print(f'{metric}: {value:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c47784",
   "metadata": {},
   "source": [
    "---\n",
    "Recommendations for User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c91cc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations:\n",
      "Kage no Jitsuryokusha ni Naritakute!\n",
      "Kaguya-sama wa Kokurasetai: Ultra Romantic\n",
      "Jujutsu Kaisen 2nd Season\n",
      "Kage no Jitsuryokusha ni Naritakute! 2nd season\n",
      "Fruits Basket: The Final\n",
      "VINLAND SAGA SEASON 2\n",
      "Mob Psycho 100\n",
      "Mob Psycho 100 II\n",
      "Kaguya-sama wa Kokurasetai: First Kiss wa Owaranai\n",
      "Akatsuki no Yona\n"
     ]
    }
   ],
   "source": [
    "user_id = 1774523\n",
    "\n",
    "recs_hybrid = recommend_for_user_hybrid(\n",
    "    recommend_for_user_cbf(ratings_df[ratings_df['userID'] == user_id], X_weighted, id_to_idx, anime_df_ids, anime_df_titles, index),\n",
    "    recommend_for_user_cf(ratings_df[ratings_df['userID'] == user_id], anime_df, als_model, anime_ids, anime_map, user_id, user_map, ratings_df),\n",
    "    anime_df,\n",
    "    alpha=alpha,\n",
    "    boost_pop=boost_pop,\n",
    "    boost_score=boost_score,\n",
    "    k=top_k \n",
    ")\n",
    "\n",
    "seen_ids = set(ratings_df[ratings_df['userID'] == user_id]['idMal'])\n",
    "recs_hybrid_unseen = recs_hybrid.loc[~recs_hybrid.index.isin(seen_ids)].head(top_k)\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "for title in recs_hybrid_unseen['title'].values:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c6155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
